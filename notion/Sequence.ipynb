{"cells":[{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74},"id":"oc1x5y_GmG-i","executionInfo":{"status":"ok","timestamp":1642063653244,"user_tz":-540,"elapsed":17231,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}},"outputId":"d69653a9-e159-473b-b569-7a1c65924614"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-b945c094-0563-4b23-a49a-f9726b17d33a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b945c094-0563-4b23-a49a-f9726b17d33a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving shakespeare.txt to shakespeare.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"gJY9dqWoleB6"},"source":["# 1. 데이터 다듬기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmTnpcABleCF","executionInfo":{"status":"ok","timestamp":1642063661309,"user_tz":-540,"elapsed":2815,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}},"outputId":"4597e4a9-49c9-4d4a-f167-39113f173d77","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["sentence[0] \"First Citizen:\"\n","sentence[1] \"Before we proceed any further, hear me speak.\"\n","sentence[2] \"\"\n","sentence[3] \"All:\"\n","sentence[4] \"Speak, speak.\"\n","sentence[5] \"\"\n","sentence[6] \"First Citizen:\"\n","sentence[7] \"You are all resolved rather to die than to famish?\"\n","sentence[8] \"\"\n"]}],"source":["import os, re \n","import numpy as np\n","import tensorflow as tf\n","\n","# 파일을 읽기모드로 열고\n","# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n","file_path = ('shakespeare.txt')\n","with open(file_path, \"r\") as f:\n","    raw_corpus = f.read().splitlines()\n","\n","# 앞에서부터 10라인만 화면에 출력해 볼까요?\n","for i, txt in enumerate(raw_corpus[:9]):\n","    print(f'sentence[{i}]', f'\"{txt}\"')\n","# print(raw_corpus[:9])"]},{"cell_type":"markdown","metadata":{"id":"EUU_kJJSleCL"},"source":["- 실습에 사용할 라이브러리를 불러온다.\n","- 준비한 txt 파일의 내용을 불러온다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLIVDieRleCM","outputId":"d45ab737-5318-498e-9603-a4363afcf242","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063663842,"user_tz":-540,"elapsed":255,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Before we proceed any further, hear me speak.\n","Speak, speak.\n","You are all resolved rather to die than to famish?\n"]}],"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: # 길이가 0인 문장은 건너뜀\n","        continue\n","    if sentence[-1] == \":\": # 문장의 끝이 : 인 문장은 건너뜀\n","        continue\n","    if idx > 9: # 문장 10개만 확인해보자\n","        break\n","\n","    print(sentence)"]},{"cell_type":"markdown","metadata":{"id":"nc88wWT-leCN"},"source":["- 원치 않는 문장인 **화자가 표기된 문장(0, 3, 6)**, 그리고 **공백인 문장(2, 5, 9)**\n","- 화자가 표기된 문장의 끝은 `:`으로 끝나고 일반적으로 대사가 끝날때 `:`로 끝나지 않으니 `:`를 기준으로 문장 제외"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STVemfEzleCN","outputId":"944a48f6-c37b-4bde-c4d0-16a3764ddb6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063665766,"user_tz":-540,"elapsed":6,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<start> this is sample sentence . <end>\n"]}],"source":["# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","    sentence = sentence.lower().strip()\n","    # 2. 특수문자 양쪽에 공백을 넣고\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n","    # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n","    # 5. 다시 양쪽 공백을 지웁니다\n","    sentence = sentence.strip()\n","    #  6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","    sentence = '<start> ' + sentence + ' <end>'\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"]},{"cell_type":"markdown","metadata":{"id":"Pss-IK_cleCP"},"source":["##### **토큰화(Tokenize)**: 텍스트 생성 모델은 단어 사전을 만들어서 문장을 일정한 기준으로 쪼개주는 것\n","- 가장 쉬운 방법은 띄어쓰기 기준으로 나누기가 있지만 약간의 문제가 있다.\n","1. Hi, my name is John. *(\"Hi,\" \"my\", ..., \"john.\" 으로 분리됨) - 문장부호\n","2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자\n","3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자\n","- 1번을 막기 위해 **문장 부호 양쪽에 공백 추가**\n","- 2번을 막기 위해 **모든 문자들을 소문자로 변환**\n","- 3번을 막기 위해 **특수문자들은 제거**\n","***\n","- 우리가 구축해야 할 데이터셋은 어떤 모양일까?\n","```\n","언어 모델의 입력 문장 : <start> 난는 밥을 먹었다\n","언어 모델의 출력 문장 : 나는 밥을 먹었다 <end>\n","```\n","> **소스 문장(Source Semtemce)**: 자연어처리 분야에서 모델에 입력되는 문장(X_train)    \n","**타겟 문장(Target Sentence)**: 정답 역할을 하게 될 모델의 출력 문장(y_train)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kMgoCPSleCQ","outputId":"487125b8-1537-4f1d-8ae6-3bd43cd6f1fa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063668245,"user_tz":-540,"elapsed":596,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> before we proceed any further , hear me speak . <end>',\n"," '<start> speak , speak . <end>',\n"," '<start> you are all resolved rather to die than to famish ? <end>',\n"," '<start> resolved . resolved . <end>',\n"," '<start> first , you know caius marcius is chief enemy to the people . <end>',\n"," '<start> we know t , we know t . <end>',\n"," '<start> let us kill him , and we ll have corn at our own price . <end>',\n"," '<start> is t a verdict ? <end>',\n"," '<start> no more talking on t let it be done away , away ! <end>',\n"," '<start> one word , good citizens . <end>']"]},"metadata":{},"execution_count":7}],"source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"]},{"cell_type":"markdown","metadata":{"id":"cf4PGnawleCR"},"source":["- `<end>`를 없애면 **소스문장**\n","- `<start>`를 없애면 **타겟문장**\n","> 인공지능의 모국어인 숫자로 표현하기위해 **배우고자 하는 언어**를 **모국어로 표현**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"706c2c2WleCT","outputId":"e6d27159-8e14-465b-92c7-ad59536c0ce8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063671182,"user_tz":-540,"elapsed":639,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2  143   40 ...    0    0    0]\n"," [   2  110    4 ...    0    0    0]\n"," [   2   11   50 ...    0    0    0]\n"," ...\n"," [   2  149 4553 ...    0    0    0]\n"," [   2   34   71 ...    0    0    0]\n"," [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f7c0ac5ba50>\n"]}],"source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=7000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"]},{"cell_type":"markdown","metadata":{"id":"YLpDTy0_leCU"},"source":["- tensorflow는 자연어 처리를 위한 여러 가지 모듈을 제공\n","- `tf.keras.preprocessing.text.Tokenizer`패키지 이용   \n","    - 이 패키지는 정제된 데이터를 토큰화 시켜줌  \n","    - 단어사전(vocabulary 또는 dictionary)을 만들어줌   \n","    - 데이터를 숫자로 변환시켜줌   \n","     \n","\n","- 이 과정을 **벡터화(vectorize)**라 하며, 숫자로 변환된 데이터를 **텐서(tensor)**라 한다.\n","> 텐서(tensor)는 어려운 물리학 및 수학 개념이다. 아래의 링크를 통해 간단한 개념을 알고가자.   \n","[Tensor란 무엇인가?](https://rekt77.tistory.com/102)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGHVwKpFleCV","outputId":"d2f4c9d5-01b8-4175-b978-a9fceb95453e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063673671,"user_tz":-540,"elapsed":250,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2  143   40  933  140  591    4  124   24  110]\n"," [   2  110    4  110    5    3    0    0    0    0]\n"," [   2   11   50   43 1201  316    9  201   74    9]]\n"]}],"source":["print(tensor[:3, :10])"]},{"cell_type":"markdown","metadata":{"id":"Ez9c_Lq8leCW"},"source":["- 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해보자.\n","- 텐서 데이터는 모두 정수로 이루어짐\n","- tokenizer에 구축된 단어 사전의 인덱스임"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmq6ZDaSleCX","outputId":"2dc5c644-65d7-4d98-f5a3-aa98a14d5e79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063675594,"user_tz":-540,"elapsed":253,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : .\n","6 : the\n","7 : and\n","8 : i\n","9 : to\n","10 : of\n"]}],"source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"]},{"cell_type":"markdown","metadata":{"id":"J_2v3Lc0leCY"},"source":["- tokenizer의 단어사전은 어떻게 구축되어 있는지 확인해보자.\n","- 2번 인덱스가 <start>이다.\n","- `tensor[:3, :10]`의 시작이 왜 2로 시작하는지 이해할 수 있겠다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoqc5F3DleCY","outputId":"c9229688-6492-4fa1-cb41-8e4955d699a3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063677669,"user_tz":-540,"elapsed":361,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n","   0   0]\n","[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n","   0   0]\n"]}],"source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0])\n","print(tgt_input[0])"]},{"cell_type":"markdown","metadata":{"id":"IJOi84NnleCZ"},"source":["- 생성된 텐서를 **소스**와 **타겟**으로 분리하여 모델이 학습할 수 있게 해주자.\n","- 텐스 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패팅(padding)을 채워 넣은 것이다.\n","- 사전에는 없지만 0은 바로 패딩 문자 `<pad>`이다.\n","***\n","- 변수로 지정한 `corpus` 내의 첫 번째 문장의 소스와 타겟 문장을 확인해보자.   \n","    - **소스**: 2`<start>`에서 시작해서 3`<end>`으로 끝난 후 0`<pad>`로 채워짐   \n","    - **타겟**: 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 한 형태를 가짐"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OV4v6UtaleCZ","outputId":"598a692f-e233-4a66-9feb-03059aac7e2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063681713,"user_tz":-540,"elapsed":239,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"]},"metadata":{},"execution_count":12}],"source":["BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"_M8QCoNbleCa"},"source":["**마지막으로 데이터셋 객체 생성**   \n","**keras의 경우 model.fit(x_train, y_train,....)형태로 학습을 진행하였다**   \n","**하지만 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 tf.data.Dataset 객체를 생성하는 방법을 사용**\n","- `tf.data.Dataset`객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공\n","- `tf.data.Dataset.from_tensor_slices()` 메소드를 이용해 `tf.data.Dataset`객체를 생성\n"]},{"cell_type":"markdown","metadata":{"id":"-8YLiht9leCb"},"source":["# 2. 인공지능 학습시키기\n","> 인공지능이라 부르는 것은 인공신경망이자 딥러닝 네트워크이자 순환신경망이자.......   \n","그냥 모델(model)이라 부르자.   \n","![RNN](C:\\Users\\zzocojoa\\Desktop\\pic_2.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd1cq0wFleCb"},"outputs":[],"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"markdown","metadata":{"id":"o-DDsflNleCc"},"source":["- 입력 텐서에는 단어 사전의 인덱스가 들어있다고 했다.\n","- Embedding레이어에는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n","- 이 워드 벡터는 의미 공간에서 단어의 추상적 표현(representation)으로 사용된다.\n","***\n","- 위 코드에서 `embeddin_size`는 워드 벡터의 차원수 즉, 다어가 추상적으로 표현되는 크기이다.   \n","(만약 크기가 2일 경우)   \n","    - 차갑다: [0.0, 1.0]   \n","    - 뜨겁다: [1.0, 0.0]   \n","    - 미지근하다: [0.5, 0.5]\n","- 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란을 만든다.\n","- 이번 모델에서는 `embeddin_size`는 256이 적당하다.\n","***\n","- LSTM 레이어의 `hidden state` 의 차원수인 hidden_size 도 같은 맥락이다.   \n","    - hidden_size 는 모델에 얼마나 많은 일꾼을 둘 것인가.   \n","    - 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데,   \n","    충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 배가 산으로 갈 뿐\n","- 이번 모델에서 `hidden state`는 1024가 적당하다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pAHAulxleCc","outputId":"24dde083-efea-4f3a-9034-fee909e90075","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063688615,"user_tz":-540,"elapsed":5060,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n","array([[[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [-4.1279459e-04,  1.9561421e-04, -4.2430585e-04, ...,\n","          1.8878248e-04, -4.1644086e-04, -1.3420604e-04],\n","        [-5.0915993e-04,  2.3712967e-04, -7.9280575e-04, ...,\n","          2.1559636e-04, -6.5501564e-04,  1.8159240e-04],\n","        ...,\n","        [ 1.9092321e-03, -2.2421991e-03,  3.3173070e-03, ...,\n","          9.0502016e-04, -2.6100739e-03,  5.9498032e-03],\n","        [ 1.9192731e-03, -2.5304379e-03,  3.4943561e-03, ...,\n","          8.9233613e-04, -2.8159029e-03,  6.4732819e-03],\n","        [ 1.9202124e-03, -2.7837725e-03,  3.6535128e-03, ...,\n","          8.9514215e-04, -2.9803335e-03,  6.9239838e-03]],\n","\n","       [[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [-3.1337997e-05,  4.6573853e-04,  2.8920374e-04, ...,\n","          2.4093908e-05,  2.0763079e-04, -1.1225071e-04],\n","        [ 1.6262927e-04,  3.1967685e-04,  7.2313752e-04, ...,\n","          2.7053378e-04,  4.0116676e-04, -3.2713436e-04],\n","        ...,\n","        [ 1.2484987e-03, -1.4145776e-03,  2.7452509e-03, ...,\n","          2.8832967e-04, -8.7305688e-04,  4.7847969e-03],\n","        [ 1.3601135e-03, -1.7585895e-03,  2.9483514e-03, ...,\n","          3.1096817e-04, -1.2432263e-03,  5.3842966e-03],\n","        [ 1.4447534e-03, -2.0664630e-03,  3.1441255e-03, ...,\n","          3.4853502e-04, -1.5883151e-03,  5.9276107e-03]],\n","\n","       [[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [-8.8920489e-05, -8.6641099e-05, -2.0984236e-04, ...,\n","          2.5208463e-04, -1.5010514e-04, -5.5269949e-04],\n","        [-3.8645533e-04, -1.3805200e-04, -1.3512331e-04, ...,\n","          9.5501862e-05, -1.6736638e-04, -7.0971425e-04],\n","        ...,\n","        [ 2.0085236e-03, -2.0554506e-03,  3.0032089e-03, ...,\n","          5.6157348e-04, -2.3316359e-03,  4.6303072e-03],\n","        [ 1.9699833e-03, -2.4181833e-03,  3.1757196e-03, ...,\n","          6.0002878e-04, -2.5663050e-03,  5.2996324e-03],\n","        [ 1.9271094e-03, -2.7256659e-03,  3.3359434e-03, ...,\n","          6.4291217e-04, -2.7659833e-03,  5.8924793e-03]],\n","\n","       ...,\n","\n","       [[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [-1.3534202e-04,  8.6338456e-05, -1.3283033e-04, ...,\n","          2.4168026e-05,  1.3452284e-04,  1.5304636e-04],\n","        [-7.0664493e-05,  5.1723648e-05, -3.2680089e-04, ...,\n","          1.6487928e-04,  2.6436173e-04,  6.8839540e-04],\n","        ...,\n","        [ 2.3824736e-03, -1.2642664e-03,  1.9559681e-03, ...,\n","          7.2876224e-04, -1.5225110e-03,  4.9978448e-03],\n","        [ 2.3765324e-03, -1.6546920e-03,  2.2195133e-03, ...,\n","          7.3928025e-04, -1.8711338e-03,  5.5802804e-03],\n","        [ 2.3429987e-03, -2.0006187e-03,  2.4787583e-03, ...,\n","          7.5483636e-04, -2.1804001e-03,  6.1058556e-03]],\n","\n","       [[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [-3.8849868e-04, -2.2935214e-04, -1.2396212e-04, ...,\n","          4.0480963e-04,  1.5683475e-04, -3.5375252e-04],\n","        [-6.5004546e-04,  1.2565393e-04, -1.4330803e-04, ...,\n","          4.4022602e-04,  9.7725140e-05, -4.2261800e-04],\n","        ...,\n","        [ 1.4470554e-03, -2.5544362e-03,  2.3696185e-03, ...,\n","          2.7658642e-04, -1.4382955e-03,  5.4529440e-03],\n","        [ 1.5271351e-03, -2.8395881e-03,  2.6318168e-03, ...,\n","          3.4163272e-04, -1.7824706e-03,  6.0494188e-03],\n","        [ 1.5906013e-03, -3.0851196e-03,  2.8729260e-03, ...,\n","          4.1610733e-04, -2.0748426e-03,  6.5643946e-03]],\n","\n","       [[-8.7577799e-05,  7.7194483e-05, -5.9036476e-05, ...,\n","          2.3434141e-04,  5.9491878e-05, -2.5918504e-04],\n","        [ 3.2294778e-05,  2.8824879e-04, -3.0752260e-04, ...,\n","          2.6922493e-04,  1.3319994e-04, -1.7627991e-04],\n","        [-6.7518675e-05,  4.0156979e-04, -5.3750176e-04, ...,\n","         -4.1899875e-05, -6.6862456e-05,  7.2006340e-05],\n","        ...,\n","        [ 1.8221138e-03, -6.8259012e-04,  1.4680168e-03, ...,\n","          1.8300611e-04, -1.5834316e-03,  5.3289966e-03],\n","        [ 1.8808561e-03, -1.1180403e-03,  1.7863180e-03, ...,\n","          2.1614748e-04, -1.9773962e-03,  5.8913743e-03],\n","        [ 1.9138054e-03, -1.5213239e-03,  2.0840932e-03, ...,\n","          2.6732174e-04, -2.3068436e-03,  6.3883225e-03]]], dtype=float32)>"]},"metadata":{},"execution_count":14}],"source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","# 지금은 동작 원리에 너무 빠져들지 마세요~\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"]},{"cell_type":"markdown","metadata":{"id":"QRkiu8XaleCd"},"source":["- model에 데이터를 조금 태워 보는 것\n","- input shape가 결정되면서 model.build()가 자동으로 호출\n","***\n","- 텐서 shape를 보면 `shape=(256, 20, 7001)`을 알 수 있다.\n","- 7001은 Dense레이어의 출력 차원수\n","    - 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링\n","- 256은 이전 스텝에서 지정한 배치 사이즈   \n","    - `dataset.take(1)`를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것\n","- 20: LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력   \n","    - `tf.keras.layers.LSTM(hidden_size, return_sequences=True)`로 호출한 LSTM 레이어에서 `return_sequences=True`이라고 지정한 부분\n","***\n","- 모델의 입력 데이터의 시퀀스 길이가 얼마인지 모르고, 모델을 만들면서 알려준 적도 없다.   \n","그럼 어떻게 알게된 것일까??   \n","    > 데이터를 입력받으면서 알게된 것이다. 데이터셋의 max_len이 20으로맞춰져 있던 것이다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KDrwGw2leCd","outputId":"914c3874-4133-42b7-94d9-3fd058d65e39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642063696722,"user_tz":-540,"elapsed":250,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  1792256   \n","                                                                 \n"," lstm (LSTM)                 multiple                  5246976   \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  8392704   \n","                                                                 \n"," dense (Dense)               multiple                  7176025   \n","                                                                 \n","=================================================================\n","Total params: 22,607,961\n","Trainable params: 22,607,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"cqeb0VyvleCd"},"source":["- Output Shape를 정확하게 알려주지 않는다.\n","- 이 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없다.\n","- 하지만 모델의 파라미터 사이즈는 측정된다.(대략 22million)\n"]},{"cell_type":"markdown","metadata":{"id":"3kOx9wOOleCe"},"source":["- Loss: 모델이 오답을 만들고 있는 정도\n","- 오답률이 감소할 수 록 **학습이 잘 진행되고 있다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRyMjB_GleCe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642064913859,"user_tz":-540,"elapsed":1215022,"user":{"displayName":"오정현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLyqEsh89TxvfIDbBiylXEsU-0wf65Dj3R3CAxQ=s64","userId":"10555307556475656913"}},"outputId":"18e6e718-440f-4d1a-8b1b-a5d2aef3d98c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","93/93 [==============================] - 44s 431ms/step - loss: 3.4686\n","Epoch 2/30\n","93/93 [==============================] - 40s 430ms/step - loss: 2.7954\n","Epoch 3/30\n","93/93 [==============================] - 40s 430ms/step - loss: 2.6776\n","Epoch 4/30\n","93/93 [==============================] - 40s 431ms/step - loss: 2.5781\n","Epoch 5/30\n","93/93 [==============================] - 40s 430ms/step - loss: 2.5176\n","Epoch 6/30\n","93/93 [==============================] - 40s 428ms/step - loss: 2.4620\n","Epoch 7/30\n","93/93 [==============================] - 40s 426ms/step - loss: 2.3990\n","Epoch 8/30\n","93/93 [==============================] - 40s 428ms/step - loss: 2.3417\n","Epoch 9/30\n","93/93 [==============================] - 40s 429ms/step - loss: 2.2851\n","Epoch 10/30\n","93/93 [==============================] - 40s 427ms/step - loss: 2.2346\n","Epoch 11/30\n","93/93 [==============================] - 40s 431ms/step - loss: 2.1814\n","Epoch 12/30\n","93/93 [==============================] - 40s 428ms/step - loss: 2.1308\n","Epoch 13/30\n","93/93 [==============================] - 40s 429ms/step - loss: 2.0789\n","Epoch 14/30\n","93/93 [==============================] - 40s 425ms/step - loss: 2.0277\n","Epoch 15/30\n","93/93 [==============================] - 40s 427ms/step - loss: 1.9752\n","Epoch 16/30\n","93/93 [==============================] - 40s 430ms/step - loss: 1.9224\n","Epoch 17/30\n","93/93 [==============================] - 40s 428ms/step - loss: 1.8689\n","Epoch 18/30\n","93/93 [==============================] - 40s 425ms/step - loss: 1.8141\n","Epoch 19/30\n","93/93 [==============================] - 40s 427ms/step - loss: 1.7584\n","Epoch 20/30\n","93/93 [==============================] - 40s 426ms/step - loss: 1.7019\n","Epoch 21/30\n","93/93 [==============================] - 40s 428ms/step - loss: 1.6466\n","Epoch 22/30\n","93/93 [==============================] - 40s 426ms/step - loss: 1.5897\n","Epoch 23/30\n","93/93 [==============================] - 39s 424ms/step - loss: 1.5323\n","Epoch 24/30\n","93/93 [==============================] - 40s 427ms/step - loss: 1.4734\n","Epoch 25/30\n","93/93 [==============================] - 40s 425ms/step - loss: 1.4154\n","Epoch 26/30\n","93/93 [==============================] - 39s 424ms/step - loss: 1.3581\n","Epoch 27/30\n","93/93 [==============================] - 40s 427ms/step - loss: 1.2978\n","Epoch 28/30\n","93/93 [==============================] - 39s 423ms/step - loss: 1.2387\n","Epoch 29/30\n","93/93 [==============================] - 40s 429ms/step - loss: 1.1798\n","Epoch 30/30\n","93/93 [==============================] - 40s 429ms/step - loss: 1.1192\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7c8000c990>"]},"metadata":{},"execution_count":16}],"source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=30)"]},{"cell_type":"code","source":[""],"metadata":{"id":"NZaRdAj-s3uG"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"f47048469d0a2729df49bd93e3bdf1708b234c0f91217dae9f99879c6ec1351f"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"colab":{"name":"Sequence.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}